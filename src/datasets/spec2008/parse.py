import json
import os
import re

import bs4 as bs
import numpy as np
import pandas as pd
import requests as req

from src.support.log import tqdm_wrapper

URL = "https://www.spec.org/power_ssj2008/results/power_ssj2008.html"


def main():
    # Get list of all pages
    pages = get_pages(URL)
    redownload = False

    print(f"Found {len(pages)} pages.")

    os.makedirs("data/spec2008", exist_ok=True)
    failed = []

    if redownload:
        for page in tqdm_wrapper(pages):
            try:
                fn = re.sub(r"[^a-zA-Z0-9]", "-", "-".join([str(i) for i in page["cpu"].values()])) \
                    .replace("--", "-") \
                    .lower()
                if fn[-1] == "-":
                    fn = fn[:-1]

                page["filename"] = fn
                os.makedirs(f"data/spec2008/{fn}", exist_ok=True)
                content = len(os.listdir(f"data/spec2008/{fn}"))

                data = parse_page(page["link"])
                page["data"] = pd.DataFrame(data).to_dict(orient="records")
                open(f"data/spec2008/{fn}/{content}.json", "w").write(json.dumps(page, indent=4))
            except Exception as e:
                failed.append(page)
                print("Error parsing page: " + page["pc_model"])
                print(e)
                continue

        open("data/spec2008/failed.json", "w").write(json.dumps(failed, indent=4))

    # Merge all files
    os.makedirs("data/spec2008_agg", exist_ok=True)

    for folder in os.listdir("data/spec2008"):
        if folder == ".DS_Store":
            continue

        if not os.path.isdir("data/spec2008/" + folder):
            print("Skipping file " + folder + " as it is not a folder.")
            continue

        if os.path.exists("data/spec2008_agg/" + folder + ".json"):
            print("Skipping folder " + folder + " as it already exists.")
            continue

        merge(folder)


def get_pages(url):
    ret = []
    r = req.get(url)
    soup = bs.BeautifulSoup(r.text, "html.parser")
    # find div with class=.resultsTable
    table = soup.find("table")
    # find all rows
    rows = table.findAll("tr")
    for row in rows:
        # save info about the model and then the link to the html file
        cols = row.findAll("td")
        if len(cols) > 0:
            ret.append({
                "pc_vendor": cols[0].text,
                "pc_model": cols[1].text.split("\n")[0],
                "link": "https://www.spec.org/power_ssj2008/results/" + cols[1].find("a")["href"],
                "cpu": {
                    "model": cols[4].text,
                    "mhz": cols[5].text,
                    "chips": cols[6].text,
                    "cores": cols[7].text,
                    "threads": cols[8].text
                },
                "memory": cols[9].text
            })
            try:
                ret[-1]["cpu"]["mhz"] = int(ret[-1]["cpu"]["mhz"])
            except ValueError:
                ret[-1]["cpu"]["mhz"] = None

            try:
                ret[-1]["cpu"]["chips"] = int(ret[-1]["cpu"]["chips"])
            except ValueError:
                ret[-1]["cpu"]["chips"] = None

            try:
                ret[-1]["cpu"]["cores"] = int(ret[-1]["cpu"]["cores"])
            except ValueError:
                ret[-1]["cpu"]["cores"] = None

            try:
                ret[-1]["cpu"]["threads"] = int(ret[-1]["cpu"]["threads"])
            except ValueError:
                ret[-1]["cpu"]["threads"] = None

            try:
                ret[-1]["memory"] = int(ret[-1]["memory"])
            except ValueError:
                ret[-1]["memory"] = None

    return ret


def parse_page(url):
    # print("...parsing page: " + url)
    r = req.get(url)
    soup = bs.BeautifulSoup(r.text, "html.parser")
    # find div with class=.resultsTable
    div = soup.find("div", {"class": "resultsTable"})
    # find table
    table = div.find("table")

    # parse header, if a column has colspan, then append the text of the next row
    header = table.find("thead")
    header_rows = header.findAll("tr")
    labels = []
    i = 0
    header_cols = header_rows[i].findAll("th")
    for j in range(len(header_cols)):
        if "colspan" in header_cols[j].attrs:
            sub = [c.text for c in header_rows[i + 1].findAll("th")]
            for k in range(int(header_cols[j]["colspan"])):
                labels.append(header_cols[j].text + "/" + sub[k])
        else:
            labels.append(header_cols[j].text)

    # parse body
    body = table.find("tbody")
    body_rows = body.findAll("tr")
    body_rows = [[c.text for c in r.findAll("td")] for r in body_rows]
    body_rows = [r for r in body_rows if len(r) == len(labels)]

    df = pd.DataFrame(body_rows, columns=labels)

    # transform % to float
    df["Performance/Target Load"] = df["Performance/Target Load"].apply(lambda x: float(x[:-1]) / 100)
    df["Performance/Actual Load"] = df["Performance/Actual Load"].apply(lambda x: float(x[:-1]) / 100)

    # transform ops to int
    df["Performance/ssj_ops"] = df["Performance/ssj_ops"].apply(lambda x: int(x.replace(",", "")))

    # transform power to float
    df["Power"] = df["Power"].apply(lambda x: float(x.replace(",", "")))

    # transform ratio to float
    df["Performance to Power Ratio"] = df["Performance to Power Ratio"].apply(lambda x: float(x.replace(",", "")))

    pd.set_option('display.max_columns', None)

    return df



def merge(folder):
    print("Merging folder " + folder)
    data = []
    for file in os.listdir("data/spec2008/" + folder):
        if not file.endswith(".json"):
            raise Exception("Skipping file " + file + " as it is not a json file.")
        data.append(json.load(open("data/spec2008/" + folder + "/" + file, "r")))

    if len(data) == 0:
        print("No data found for folder " + folder)
        return

    # Verify that all data is the same, convert to lower case
    for key in data[0]["cpu"].keys():
        if type(data[0]["cpu"][key]) == str:
            target = re.sub(r"[^a-zA-Z0-9]", "-", data[0]["cpu"][key]).replace("--", "-").lower()
            check = [re.sub(r"[^a-zA-Z0-9]", "-", data[i]["cpu"][key]).replace("--", "-").lower() == target
                     for i in range(len(data)) if i > 0]
        else:
            check = [data[i]["cpu"][key] == data[0]["cpu"][key] for i in range(len(data)) if i > 0]

        if not all(check):
            raise Exception(f"CPU Data is not the same for folder {folder}: {check} for {key}")

    res = data[0].copy()
    res["data"] = {
        "Performance to Power Ratio": np.round([
            np.mean([data[i]["data"][j]["Performance to Power Ratio"] for i in range(len(data))])
            for j in range(len(data[0]["data"]))
        ], 3).tolist(),
        "Performance/Actual Load": np.round([
            np.mean([data[i]["data"][j]["Performance/Actual Load"] for i in range(len(data))])
            for j in range(len(data[0]["data"]))
        ], 3).tolist(),
        "Performance/Target Load": [data[0]["data"][i]["Performance/Target Load"]
                                    for i in range(len(data[0]["data"]))],
        "Performance/ssj_ops": np.round([
            np.mean([data[i]["data"][j]["Performance/ssj_ops"] for i in range(len(data))])
            for j in range(len(data[0]["data"]))
        ], 3).tolist(),
        "Power": np.round([
            np.mean([data[i]["data"][j]["Power"] for i in range(len(data))])
            for j in range(len(data[0]["data"]))
        ], 3).tolist(),
    }

    res["count"] = len(data)

    json.dump(res, open("data/spec2008_agg/" + folder + ".json", "w"), indent=4)


if __name__ == "__main__":
    main()